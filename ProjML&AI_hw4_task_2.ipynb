{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg2MtGS54W2yPpA7aOzPBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keran-w/Projects-in-ML-and-AI/blob/main/ProjML%26AI_hw4_task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "01BQknJCOAoX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# reference: https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "metadata": {
        "id": "MPQvxs6-OuEs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get user input\n",
        "text1 = input('text1: ')\n",
        "text2 = input('text2: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7xeAMoZSyBa",
        "outputId": "3a67d4c1-b65c-42d4-a6db-99ebbbfb27d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text1: analytics\n",
            "text2: basics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "text1_embedding = embeddings_dict[text1].reshape(1,-1)\n",
        "text2_embedding = embeddings_dict[text2].reshape(1,-1)\n",
        "# similarity score -- cosine similarity\n",
        "similarity_score = metrics.pairwise.cosine_similarity(text1_embedding, text2_embedding).item()\n",
        "# dissimilarity score -- euclidean distance\n",
        "dissimilarity_score = metrics.pairwise.euclidean_distances(text1_embedding, text2_embedding).item()\n",
        "print(f'similarity score: {similarity_score:.3f}, dissimilarity score: {dissimilarity_score:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hPcNzVjQrKX",
        "outputId": "e1a8f071-e0b5-4523-a997-b8b7414b89f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity score: 0.143, dissimilarity score: 6.156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two methods to evaluate the dissimilarity of two words. The first one is to use cosine similarity, and the higher this score is, the less the two words are dissimilar. Another method is to use euclidean distance as a dissimilar score, and the higher this score is, the more the two words are dissimilar."
      ],
      "metadata": {
        "id": "Bomp4wptaeJv"
      }
    }
  ]
}