{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAnGf/y9wd604NlLCi3Fgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keran-w/Projects-in-ML-and-AI/blob/main/ProjML%26AI_hw5_task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-QeZ-T2VMEU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!git clone https://github.com/AntixK/PyTorch-VAE\n",
        "%cd PyTorch-VAE\n",
        "!pip install -r requirements.txt\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class CVAE(nn.Module):\n",
        "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "    \n",
        "    \n",
        "        super(CVAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.LazyConv2d(32, kernel_size=5, padding=2), \n",
        "            nn.Relu(),\n",
        "            nn.LazyConv2d(64, kernel_size=5), \n",
        "            nn.Relu(),\n",
        "            nn.Flatten(1),\n",
        "            nn.LazyLinear(128), \n",
        "            nn.Relu(), # no activation\n",
        "            nn.LazyLinear(latent_dim + latent_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.LazyLinear(7*7*32),\n",
        "            nn.Relu(),\n",
        "            \n",
        "\n",
        "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=32, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            # No activation\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
        "        )\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.decoder(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits"
      ],
      "metadata": {
        "id": "IZPvGgQaVgQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}